{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc404b6-dca0-47e8-a14d-4ffde8c56d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /FileStore/tables/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22b09ca5-f895-4ef0-a185-63eddb8f17c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reading data from a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "483f8f12-db1c-4471-90ac-055ba9477ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flightData2015 = spark.read.format(\"csv\")\\\n",
    ".option(\"inferSchema\",\"false\")\\\n",
    ".option(\"header\",\"false\")\\\n",
    ".option(\"mode\",\"FAILFAST\")\\\n",
    ".load(\"/FileStore/tables/2012_summary-2.csv\")\n",
    "\n",
    "flightData2015.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "194c31c1-6798-455c-acb3-3855e98b8a12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flightData2 = spark.read.format(\"csv\")\\\n",
    ".option(\"inferSchema\",\"false\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"mode\",\"FAILFAST\")\\\n",
    ".load(\"/FileStore/tables/2012_summary-2.csv\")\n",
    "\n",
    "flightData2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec627d7-b677-49cc-805f-dc88e3f18b73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flightData2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24286a56-76fd-4b70-96f8-bf8b7b691413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flightData3 = spark.read.format(\"csv\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"mode\",\"FAILFAST\")\\\n",
    ".load(\"/FileStore/tables/2012_summary-2.csv\")\n",
    "\n",
    "flightData3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb98375-15ac-4772-8e5a-27372db9d45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flightData3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9375a7-0f25-4c94-9a07-24499605f6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType,StringType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d84841-b395-49a4-8d33-f12a61d8599e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema=StructType([StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "                      StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "                      StructField(\"count\",IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d83c332-592f-4e80-8490-214e01879795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flightData3 = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"false\")\\\n",
    ".option(\"mode\",\"permissive\")\\\n",
    ".option(\"skipRows\",1)\\\n",
    ".schema(my_schema)\\\n",
    ".load(\"/FileStore/tables/2012_summary-2.csv\")\n",
    "\n",
    "flightData3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d376471-7ed7-414f-81e1-2b68abf093ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "/FileStore/tables/2015_summary.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d05a52-08dc-4472-99a0-3cb4ac8d8f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|   15|\n|       United States|            Croatia|    1|\n|       United States|            Ireland|  344|\n|               Egypt|      United States|   15|\n|       United States|              India|   62|\n|       United States|          Singapore|    1|\n|       United States|            Grenada|   62|\n|          Costa Rica|      United States|  588|\n|             Senegal|      United States|   40|\n|             Moldova|      United States|    1|\n|       United States|       Sint Maarten|  325|\n|       United States|   Marshall Islands|   39|\n|              Guyana|      United States|   64|\n|               Malta|      United States|    1|\n|            Anguilla|      United States|   41|\n|             Bolivia|      United States|   30|\n|       United States|           Paraguay|    6|\n|             Algeria|      United States|    4|\n|Turks and Caicos ...|      United States|  230|\n|       United States|          Gibraltar|    1|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "jsonData = spark.read.format(\"json\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"mode\",\"permissive\")\\\n",
    ".option(\"inferschema\",\"false\")\\\n",
    ".load(\"/FileStore/tables/2015_summary.json\")\n",
    "\n",
    "jsonData.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c6c2803-ea55-4a71-a55b-d904fe9bec05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##lets we want to write prvious dataframe or result dataframe data  in spark\n",
    "jsonData.write.format(\"csv\")\\\n",
    "    .option(\"mode\",\"overwrite\")\\\n",
    "    .option(\"path\",\"/FileStore/tables/written_data_2015_summary\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e40779-be50-4af1-8e96-4be925bf40b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [FileInfo(path='dbfs:/FileStore/tables/written_data_2015_summary/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1736437148000),\n FileInfo(path='dbfs:/FileStore/tables/written_data_2015_summary/_committed_8388248415064428292', name='_committed_8388248415064428292', size=111, modificationTime=1736437148000),\n FileInfo(path='dbfs:/FileStore/tables/written_data_2015_summary/_started_8388248415064428292', name='_started_8388248415064428292', size=0, modificationTime=1736437147000),\n FileInfo(path='dbfs:/FileStore/tables/written_data_2015_summary/part-00000-tid-8388248415064428292-2bcdfc2a-ff34-46e7-9c78-aff46d8b6f37-6-1-c000.csv', name='part-00000-tid-8388248415064428292-2bcdfc2a-ff34-46e7-9c78-aff46d8b6f37-6-1-c000.csv', size=7036, modificationTime=1736437148000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/written_data_2015_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e1a292-31a9-4043-af77-6336be945469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## partitioning and bucketing\n",
    "jsonData.write.format(\"csv\")\\\n",
    "    .option(\"mode\",\"overwrite\")\\\n",
    "    .option(\"path\",\"/FileStore/tables/partition_written_data_2015_summary\")\\\n",
    "    .partitionBy(\"ORIGIN_COUNTRY_NAME\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ac1943-c626-4985-b0c3-39986be05358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: [FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Angola/', name='ORIGIN_COUNTRY_NAME=Angola/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Anguilla/', name='ORIGIN_COUNTRY_NAME=Anguilla/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Antigua and Barbuda/', name='ORIGIN_COUNTRY_NAME=Antigua and Barbuda/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Argentina/', name='ORIGIN_COUNTRY_NAME=Argentina/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Aruba/', name='ORIGIN_COUNTRY_NAME=Aruba/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Australia/', name='ORIGIN_COUNTRY_NAME=Australia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Austria/', name='ORIGIN_COUNTRY_NAME=Austria/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Azerbaijan/', name='ORIGIN_COUNTRY_NAME=Azerbaijan/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Bahrain/', name='ORIGIN_COUNTRY_NAME=Bahrain/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Barbados/', name='ORIGIN_COUNTRY_NAME=Barbados/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Belgium/', name='ORIGIN_COUNTRY_NAME=Belgium/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Belize/', name='ORIGIN_COUNTRY_NAME=Belize/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Bermuda/', name='ORIGIN_COUNTRY_NAME=Bermuda/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Bolivia/', name='ORIGIN_COUNTRY_NAME=Bolivia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Bonaire, Sint Eustatius, and Saba/', name='ORIGIN_COUNTRY_NAME=Bonaire, Sint Eustatius, and Saba/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Brazil/', name='ORIGIN_COUNTRY_NAME=Brazil/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=British Virgin Islands/', name='ORIGIN_COUNTRY_NAME=British Virgin Islands/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Bulgaria/', name='ORIGIN_COUNTRY_NAME=Bulgaria/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Canada/', name='ORIGIN_COUNTRY_NAME=Canada/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Cape Verde/', name='ORIGIN_COUNTRY_NAME=Cape Verde/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Cayman Islands/', name='ORIGIN_COUNTRY_NAME=Cayman Islands/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Chile/', name='ORIGIN_COUNTRY_NAME=Chile/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=China/', name='ORIGIN_COUNTRY_NAME=China/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Colombia/', name='ORIGIN_COUNTRY_NAME=Colombia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Cook Islands/', name='ORIGIN_COUNTRY_NAME=Cook Islands/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Costa Rica/', name='ORIGIN_COUNTRY_NAME=Costa Rica/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Croatia/', name='ORIGIN_COUNTRY_NAME=Croatia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Cuba/', name='ORIGIN_COUNTRY_NAME=Cuba/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Curacao/', name='ORIGIN_COUNTRY_NAME=Curacao/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Cyprus/', name='ORIGIN_COUNTRY_NAME=Cyprus/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Czech Republic/', name='ORIGIN_COUNTRY_NAME=Czech Republic/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Denmark/', name='ORIGIN_COUNTRY_NAME=Denmark/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Dominica/', name='ORIGIN_COUNTRY_NAME=Dominica/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Dominican Republic/', name='ORIGIN_COUNTRY_NAME=Dominican Republic/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Ecuador/', name='ORIGIN_COUNTRY_NAME=Ecuador/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Egypt/', name='ORIGIN_COUNTRY_NAME=Egypt/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=El Salvador/', name='ORIGIN_COUNTRY_NAME=El Salvador/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Estonia/', name='ORIGIN_COUNTRY_NAME=Estonia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Ethiopia/', name='ORIGIN_COUNTRY_NAME=Ethiopia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Federated States of Micronesia/', name='ORIGIN_COUNTRY_NAME=Federated States of Micronesia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Fiji/', name='ORIGIN_COUNTRY_NAME=Fiji/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Finland/', name='ORIGIN_COUNTRY_NAME=Finland/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=France/', name='ORIGIN_COUNTRY_NAME=France/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=French Polynesia/', name='ORIGIN_COUNTRY_NAME=French Polynesia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Georgia/', name='ORIGIN_COUNTRY_NAME=Georgia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Germany/', name='ORIGIN_COUNTRY_NAME=Germany/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Ghana/', name='ORIGIN_COUNTRY_NAME=Ghana/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Gibraltar/', name='ORIGIN_COUNTRY_NAME=Gibraltar/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Greece/', name='ORIGIN_COUNTRY_NAME=Greece/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Greenland/', name='ORIGIN_COUNTRY_NAME=Greenland/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Grenada/', name='ORIGIN_COUNTRY_NAME=Grenada/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Guadeloupe/', name='ORIGIN_COUNTRY_NAME=Guadeloupe/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Guatemala/', name='ORIGIN_COUNTRY_NAME=Guatemala/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Guyana/', name='ORIGIN_COUNTRY_NAME=Guyana/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Haiti/', name='ORIGIN_COUNTRY_NAME=Haiti/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Honduras/', name='ORIGIN_COUNTRY_NAME=Honduras/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Hong Kong/', name='ORIGIN_COUNTRY_NAME=Hong Kong/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Hungary/', name='ORIGIN_COUNTRY_NAME=Hungary/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Iceland/', name='ORIGIN_COUNTRY_NAME=Iceland/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=India/', name='ORIGIN_COUNTRY_NAME=India/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Indonesia/', name='ORIGIN_COUNTRY_NAME=Indonesia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Ireland/', name='ORIGIN_COUNTRY_NAME=Ireland/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Israel/', name='ORIGIN_COUNTRY_NAME=Israel/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Italy/', name='ORIGIN_COUNTRY_NAME=Italy/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Jamaica/', name='ORIGIN_COUNTRY_NAME=Jamaica/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Japan/', name='ORIGIN_COUNTRY_NAME=Japan/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Jordan/', name='ORIGIN_COUNTRY_NAME=Jordan/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Kiribati/', name='ORIGIN_COUNTRY_NAME=Kiribati/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Kuwait/', name='ORIGIN_COUNTRY_NAME=Kuwait/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Latvia/', name='ORIGIN_COUNTRY_NAME=Latvia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Liberia/', name='ORIGIN_COUNTRY_NAME=Liberia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Lithuania/', name='ORIGIN_COUNTRY_NAME=Lithuania/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Luxembourg/', name='ORIGIN_COUNTRY_NAME=Luxembourg/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Malaysia/', name='ORIGIN_COUNTRY_NAME=Malaysia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Malta/', name='ORIGIN_COUNTRY_NAME=Malta/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Marshall Islands/', name='ORIGIN_COUNTRY_NAME=Marshall Islands/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Martinique/', name='ORIGIN_COUNTRY_NAME=Martinique/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Mexico/', name='ORIGIN_COUNTRY_NAME=Mexico/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Montenegro/', name='ORIGIN_COUNTRY_NAME=Montenegro/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Morocco/', name='ORIGIN_COUNTRY_NAME=Morocco/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Namibia/', name='ORIGIN_COUNTRY_NAME=Namibia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Netherlands/', name='ORIGIN_COUNTRY_NAME=Netherlands/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=New Zealand/', name='ORIGIN_COUNTRY_NAME=New Zealand/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Nicaragua/', name='ORIGIN_COUNTRY_NAME=Nicaragua/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Nigeria/', name='ORIGIN_COUNTRY_NAME=Nigeria/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Norway/', name='ORIGIN_COUNTRY_NAME=Norway/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Pakistan/', name='ORIGIN_COUNTRY_NAME=Pakistan/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Palau/', name='ORIGIN_COUNTRY_NAME=Palau/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Panama/', name='ORIGIN_COUNTRY_NAME=Panama/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Papua New Guinea/', name='ORIGIN_COUNTRY_NAME=Papua New Guinea/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Paraguay/', name='ORIGIN_COUNTRY_NAME=Paraguay/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Peru/', name='ORIGIN_COUNTRY_NAME=Peru/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Philippines/', name='ORIGIN_COUNTRY_NAME=Philippines/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Poland/', name='ORIGIN_COUNTRY_NAME=Poland/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Portugal/', name='ORIGIN_COUNTRY_NAME=Portugal/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Qatar/', name='ORIGIN_COUNTRY_NAME=Qatar/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Romania/', name='ORIGIN_COUNTRY_NAME=Romania/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Russia/', name='ORIGIN_COUNTRY_NAME=Russia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Saint Barthelemy/', name='ORIGIN_COUNTRY_NAME=Saint Barthelemy/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Saint Kitts and Nevis/', name='ORIGIN_COUNTRY_NAME=Saint Kitts and Nevis/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Saint Lucia/', name='ORIGIN_COUNTRY_NAME=Saint Lucia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Samoa/', name='ORIGIN_COUNTRY_NAME=Samoa/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Saudi Arabia/', name='ORIGIN_COUNTRY_NAME=Saudi Arabia/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Senegal/', name='ORIGIN_COUNTRY_NAME=Senegal/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Singapore/', name='ORIGIN_COUNTRY_NAME=Singapore/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Sint Maarten/', name='ORIGIN_COUNTRY_NAME=Sint Maarten/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=South Africa/', name='ORIGIN_COUNTRY_NAME=South Africa/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=South Korea/', name='ORIGIN_COUNTRY_NAME=South Korea/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Spain/', name='ORIGIN_COUNTRY_NAME=Spain/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Suriname/', name='ORIGIN_COUNTRY_NAME=Suriname/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Sweden/', name='ORIGIN_COUNTRY_NAME=Sweden/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Switzerland/', name='ORIGIN_COUNTRY_NAME=Switzerland/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Taiwan/', name='ORIGIN_COUNTRY_NAME=Taiwan/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Thailand/', name='ORIGIN_COUNTRY_NAME=Thailand/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=The Bahamas/', name='ORIGIN_COUNTRY_NAME=The Bahamas/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Trinidad and Tobago/', name='ORIGIN_COUNTRY_NAME=Trinidad and Tobago/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Turkey/', name='ORIGIN_COUNTRY_NAME=Turkey/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Turks and Caicos Islands/', name='ORIGIN_COUNTRY_NAME=Turks and Caicos Islands/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Ukraine/', name='ORIGIN_COUNTRY_NAME=Ukraine/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=United Arab Emirates/', name='ORIGIN_COUNTRY_NAME=United Arab Emirates/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=United Kingdom/', name='ORIGIN_COUNTRY_NAME=United Kingdom/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=United States/', name='ORIGIN_COUNTRY_NAME=United States/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Uruguay/', name='ORIGIN_COUNTRY_NAME=Uruguay/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Venezuela/', name='ORIGIN_COUNTRY_NAME=Venezuela/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/ORIGIN_COUNTRY_NAME=Vietnam/', name='ORIGIN_COUNTRY_NAME=Vietnam/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/tables/partition_written_data_2015_summary/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1736440824000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/partition_written_data_2015_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b161d17-cc7d-4a4f-83bc-56988cc62106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## bucketing\n",
    "jsonData.write.format(\"csv\")\\\n",
    "    .option(\"mode\",\"overwrite\")\\\n",
    "    .option(\"path\",\"/FileStore/tables/bucket_written_data_2015_summary\")\\\n",
    "    .bucketBy(10,\"ORIGIN_COUNTRY_NAME\")\\\n",
    "    .saveAsTable(\"bucket_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7865defa-5493-4b47-aeaa-9d3e542bb746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: [FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1736441013000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/_committed_6881050999538627537', name='_committed_6881050999538627537', size=965, modificationTime=1736441012000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/_started_6881050999538627537', name='_started_6881050999538627537', size=0, modificationTime=1736441011000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-10_00009.c000.csv', name='part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-10_00009.c000.csv', size=3918, modificationTime=1736441012000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-1_00000.c000.csv', name='part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-1_00000.c000.csv', size=363, modificationTime=1736441011000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-2_00001.c000.csv', name='part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-2_00001.c000.csv', size=277, modificationTime=1736441011000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-3_00002.c000.csv', name='part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-3_00002.c000.csv', size=331, modificationTime=1736441011000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-4_00003.c000.csv', name='part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-4_00003.c000.csv', size=377, modificationTime=1736441011000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-5_00004.c000.csv', name='part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-5_00004.c000.csv', size=254, modificationTime=1736441011000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-6_00005.c000.csv', name='part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-6_00005.c000.csv', size=487, modificationTime=1736441012000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-7_00006.c000.csv', name='part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-7_00006.c000.csv', size=313, modificationTime=1736441012000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-8_00007.c000.csv', name='part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-8_00007.c000.csv', size=350, modificationTime=1736441012000),\n FileInfo(path='dbfs:/FileStore/tables/bucket_written_data_2015_summary/part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-9_00008.c000.csv', name='part-00000-tid-6881050999538627537-fcc57243-2e49-4c42-afc5-208407cbc758-10-9_00008.c000.csv', size=366, modificationTime=1736441012000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/bucket_written_data_2015_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085b2fd8-b02a-40a8-abf6-f400f0022f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#how to create a dataframe in spark\n",
    "#data\n",
    "\n",
    "my_data=[('suraj', '1'),('vaibhav','2'),('debojity','12'),('ravi','2'),('jay','4')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b0aa92-85aa-48d9-8b8a-e959da93ce4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#schema\n",
    "my_schema1=['name','number_rc' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36b99ea-c605-4c7a-816b-7fc8b410011f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n|    name|number_rc|\n+--------+---------+\n|   suraj|        1|\n| vaibhav|        2|\n|debojity|       12|\n|    ravi|        2|\n|     jay|        4|\n+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(data=my_data,schema=my_schema1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2515fa1-42db-4770-b9d0-9ba6cb74405e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n|   name|number_rc|\n+-------+---------+\n| sooraj|        1|\n|vaibhav|        2|\n|debojit|       12|\n|   ravi|        2|\n|    jay|        4|\n+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# cretae dataframe with schema which is having other data types too\n",
    "my_data2=[('sooraj', 1),('vaibhav',2),('debojit',12),('ravi',2),('jay',4)]\n",
    "\n",
    "my_schema2=StructType([StructField(\"name\",StringType(),True),\n",
    "                      StructField(\"number_rc\",IntegerType(),True)])\n",
    "    \n",
    "spark.createDataFrame(data=my_data2,schema=my_schema2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0115af1-0885-4c4c-bb74-c68efebb876c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|   DEST_COUNTRY_NAME|\n+--------------------+\n|       United States|\n|       United States|\n|               Egypt|\n|       United States|\n|       United States|\n|       United States|\n|          Costa Rica|\n|             Senegal|\n|              Guyana|\n|       United States|\n|       United States|\n|             Bolivia|\n|            Anguilla|\n|       United States|\n|       United States|\n|Turks and Caicos ...|\n|Saint Vincent and...|\n|               Italy|\n|            Pakistan|\n|       United States|\n+--------------------+\nonly showing top 20 rows\n\n+--------------------+-------------------+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+--------------------+-------------------+\n|       United States|            Croatia|\n|       United States|            Ireland|\n|               Egypt|      United States|\n|       United States|              India|\n|       United States|          Singapore|\n|       United States|            Grenada|\n|          Costa Rica|      United States|\n|             Senegal|      United States|\n|              Guyana|      United States|\n|       United States|   Marshall Islands|\n|       United States|       Sint Maarten|\n|             Bolivia|      United States|\n|            Anguilla|      United States|\n|       United States|           Paraguay|\n|       United States|        Afghanistan|\n|Turks and Caicos ...|      United States|\n|Saint Vincent and...|      United States|\n|               Italy|      United States|\n|            Pakistan|      United States|\n|       United States|             Russia|\n+--------------------+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#how to select columns from  dataframes\n",
    "flightData3 = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"mode\",\"permissive\")\\\n",
    ".option(\"inferschema\",\"true\")\\\n",
    ".load(\"/FileStore/tables/2012_summary-2.csv\")\n",
    "\n",
    "# string method\n",
    "flightData3.select(\"DEST_COUNTRY_NAME\").show()\n",
    "flightData3.select(\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abde7a39-8b15-499f-987f-6273fd48921d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3150585391278855>:5\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\u001B[0;32m----> 5\u001B[0m flightData3\u001B[38;5;241m.\u001B[39mselect(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEST_COUNTRY_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m),col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mORIGIN_COUNTRY_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m      6\u001B[0m flightData3\u001B[38;5;241m.\u001B[39mselect(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEST_COUNTRY_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m),col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mORIGIN_COUNTRY_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m),col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m),col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m10\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'flightData3' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3150585391278855>:5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtypes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m----> 5\u001B[0m flightData3\u001B[38;5;241m.\u001B[39mselect(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEST_COUNTRY_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m),col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mORIGIN_COUNTRY_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m      6\u001B[0m flightData3\u001B[38;5;241m.\u001B[39mselect(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEST_COUNTRY_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m),col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mORIGIN_COUNTRY_NAME\u001B[39m\u001B[38;5;124m\"\u001B[39m),col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m),col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m10\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'flightData3' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'flightData3' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#col method of selecting columns \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "flightData3.select(col(\"DEST_COUNTRY_NAME\"),col(\"ORIGIN_COUNTRY_NAME\")).show()\n",
    "flightData3.select(col(\"DEST_COUNTRY_NAME\"),col(\"ORIGIN_COUNTRY_NAME\"),col(\"count\"),col(\"count\")+10).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1adf916b-3dea-4b36-b1fc-a1da048d7621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+-------------------+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|ORIGIN_COUNTRY_NAME|\n+--------------------+-------------------+-----+-------------------+\n|       United States|            Croatia|    1|            Croatia|\n|       United States|            Ireland|  252|            Ireland|\n|               Egypt|      United States|   13|      United States|\n|       United States|              India|   62|              India|\n|       United States|          Singapore|   25|          Singapore|\n|       United States|            Grenada|   46|            Grenada|\n|          Costa Rica|      United States|  522|      United States|\n|             Senegal|      United States|   31|      United States|\n|              Guyana|      United States|   65|      United States|\n|       United States|   Marshall Islands|   30|   Marshall Islands|\n|       United States|       Sint Maarten|  245|       Sint Maarten|\n|             Bolivia|      United States|   35|      United States|\n|            Anguilla|      United States|   19|      United States|\n|       United States|           Paraguay|    5|           Paraguay|\n|       United States|        Afghanistan|    5|        Afghanistan|\n|Turks and Caicos ...|      United States|  183|      United States|\n|Saint Vincent and...|      United States|    6|      United States|\n|               Italy|      United States|  381|      United States|\n|            Pakistan|      United States|   12|      United States|\n|       United States|             Russia|  148|             Russia|\n+--------------------+-------------------+-----+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#others ways also of selecting columns \n",
    "flightData3.select(\"DEST_COUNTRY_NAME\",col(\"ORIGIN_COUNTRY_NAME\"),flightData3[\"count\"],flightData3.ORIGIN_COUNTRY_NAME).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a8bd1f-30e1-47b4-a9d0-2294527d966c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n|        country_name|          concat_add|plus_count|\n+--------------------+--------------------+----------+\n|       United States|      United States1|        11|\n|       United States|    United States252|       262|\n|               Egypt|             Egypt13|        23|\n|       United States|     United States62|        72|\n|       United States|     United States25|        35|\n|       United States|     United States46|        56|\n|          Costa Rica|       Costa Rica522|       532|\n|             Senegal|           Senegal31|        41|\n|              Guyana|            Guyana65|        75|\n|       United States|     United States30|        40|\n|       United States|    United States245|       255|\n|             Bolivia|           Bolivia35|        45|\n|            Anguilla|          Anguilla19|        29|\n|       United States|      United States5|        15|\n|       United States|      United States5|        15|\n|Turks and Caicos ...|Turks and Caicos ...|       193|\n|Saint Vincent and...|Saint Vincent and...|        16|\n|               Italy|            Italy381|       391|\n|            Pakistan|          Pakistan12|        22|\n|       United States|    United States148|       158|\n+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#expression\n",
    "#if we want to do some sql expression or transformation or change the column name with string method of selecting column the we will be using expression\n",
    "#.expr()\n",
    "flightData3.select(expr(\"DEST_COUNTRY_NAME as country_name\"),expr(\"DEST_COUNTRY_NAME||count as concat_add\"),expr(\"count+10 as plus_count\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48252c65-9309-4a8c-840b-ca5575134984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n|   DEST_COUNTRY_NAME|       origin_cn|\n+--------------------+----------------+\n|       United States|         Croatia|\n|       United States|         Ireland|\n|               Egypt|   United States|\n|       United States|           India|\n|       United States|       Singapore|\n|       United States|         Grenada|\n|          Costa Rica|   United States|\n|             Senegal|   United States|\n|              Guyana|   United States|\n|       United States|Marshall Islands|\n|       United States|    Sint Maarten|\n|             Bolivia|   United States|\n|            Anguilla|   United States|\n|       United States|        Paraguay|\n|       United States|     Afghanistan|\n|Turks and Caicos ...|   United States|\n|Saint Vincent and...|   United States|\n|               Italy|   United States|\n|            Pakistan|   United States|\n|       United States|          Russia|\n+--------------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# column alising through col(\"\") method\n",
    "flightData3.select(col(\"DEST_COUNTRY_NAME\"),col(\"ORIGIN_COUNTRY_NAME\").alias(\"origin_cn\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32237669-0516-4151-81a7-797c480c95b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|   DEST_COUNTRY_NAME|count|\n+--------------------+-----+\n|       United States|  252|\n|       United States|   62|\n|       United States|   25|\n|       United States|   46|\n|          Costa Rica|  522|\n|             Senegal|   31|\n|              Guyana|   65|\n|       United States|   30|\n|       United States|  245|\n|             Bolivia|   35|\n|Turks and Caicos ...|  183|\n|               Italy|  381|\n|       United States|  148|\n|       United States|   63|\n|       United States|  607|\n|             Iceland|  137|\n|    Marshall Islands|   60|\n|          Luxembourg|  111|\n|            Honduras|  413|\n|         The Bahamas|  975|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#filtering columns from  dataframes\n",
    "flightData3 = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"mode\",\"permissive\")\\\n",
    ".option(\"inferschema\",\"true\")\\\n",
    ".load(\"/FileStore/tables/2012_summary-2.csv\")\n",
    "\n",
    "\n",
    "flightData3.select(col(\"DEST_COUNTRY_NAME\"),col(\"count\")).filter(col(\"count\")>20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ffca955-1c38-405a-b471-0deae0f74a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|   DEST_COUNTRY_NAME|count|\n+--------------------+-----+\n|       United States|  252|\n|       United States|   62|\n|       United States|   25|\n|       United States|   46|\n|          Costa Rica|  522|\n|             Senegal|   31|\n|              Guyana|   65|\n|       United States|   30|\n|       United States|  245|\n|             Bolivia|   35|\n|Turks and Caicos ...|  183|\n|               Italy|  381|\n|       United States|  148|\n|       United States|   63|\n|       United States|  607|\n|             Iceland|  137|\n|    Marshall Islands|   60|\n|          Luxembourg|  111|\n|            Honduras|  413|\n|         The Bahamas|  975|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "flightData3.select(col(\"DEST_COUNTRY_NAME\"),col(\"count\")).filter((col(\"count\")>20) & (col(\"DEST_COUNTRY_NAME\")!=\"india\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b3512a-80c1-429d-a73f-0ad3a297df10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|   DEST_COUNTRY_NAME|count|\n+--------------------+-----+\n|       United States|  252|\n|       United States|   62|\n|       United States|   25|\n|       United States|   46|\n|          Costa Rica|  522|\n|             Senegal|   31|\n|              Guyana|   65|\n|       United States|   30|\n|       United States|  245|\n|             Bolivia|   35|\n|Turks and Caicos ...|  183|\n|               Italy|  381|\n|       United States|  148|\n|       United States|   63|\n|       United States|  607|\n|             Iceland|  137|\n|    Marshall Islands|   60|\n|          Luxembourg|  111|\n|            Honduras|  413|\n|         The Bahamas|  975|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#filter and where are same we can use any\n",
    "flightData3.select(col(\"DEST_COUNTRY_NAME\"),col(\"count\")).where(col(\"count\")>20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e399756-cfac-499e-97b4-4957d0732d19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+\n|   DEST_COUNTRY_NAME|count|all the best|\n+--------------------+-----+------------+\n|       United States|    1|all the best|\n|       United States|  252|all the best|\n|               Egypt|   13|all the best|\n|       United States|   62|all the best|\n|       United States|   25|all the best|\n|       United States|   46|all the best|\n|          Costa Rica|  522|all the best|\n|             Senegal|   31|all the best|\n|              Guyana|   65|all the best|\n|       United States|   30|all the best|\n|       United States|  245|all the best|\n|             Bolivia|   35|all the best|\n|            Anguilla|   19|all the best|\n|       United States|    5|all the best|\n|       United States|    5|all the best|\n|Turks and Caicos ...|  183|all the best|\n|Saint Vincent and...|    6|all the best|\n|               Italy|  381|all the best|\n|            Pakistan|   12|all the best|\n|       United States|  148|all the best|\n+--------------------+-----+------------+\nonly showing top 20 rows\n\n+--------------------+-----+------------+\n|   DEST_COUNTRY_NAME|count|  new_column|\n+--------------------+-----+------------+\n|       United States|    1|all the best|\n|       United States|  252|all the best|\n|               Egypt|   13|all the best|\n|       United States|   62|all the best|\n|       United States|   25|all the best|\n|       United States|   46|all the best|\n|          Costa Rica|  522|all the best|\n|             Senegal|   31|all the best|\n|              Guyana|   65|all the best|\n|       United States|   30|all the best|\n|       United States|  245|all the best|\n|             Bolivia|   35|all the best|\n|            Anguilla|   19|all the best|\n|       United States|    5|all the best|\n|       United States|    5|all the best|\n|Turks and Caicos ...|  183|all the best|\n|Saint Vincent and...|    6|all the best|\n|               Italy|  381|all the best|\n|            Pakistan|   12|all the best|\n|       United States|  148|all the best|\n+--------------------+-----+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#literal\n",
    "# by literal we can add any extra column and we can give value for in those columns \n",
    "flightData3.select(col(\"DEST_COUNTRY_NAME\"),col(\"count\"),lit(\"all the best\")).show()\n",
    "flightData3.select(col(\"DEST_COUNTRY_NAME\"),col(\"count\"),lit(\"all the best\").alias(\"new_column\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec2bc5b7-3de8-41f0-8aff-0ea61b442543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+------------+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|      wishes|\n+--------------------+-------------------+-----+------------+\n|       United States|            Croatia|    1|all the best|\n|       United States|            Ireland|  252|all the best|\n|               Egypt|      United States|   13|all the best|\n|       United States|              India|   62|all the best|\n|       United States|          Singapore|   25|all the best|\n|       United States|            Grenada|   46|all the best|\n|          Costa Rica|      United States|  522|all the best|\n|             Senegal|      United States|   31|all the best|\n|              Guyana|      United States|   65|all the best|\n|       United States|   Marshall Islands|   30|all the best|\n|       United States|       Sint Maarten|  245|all the best|\n|             Bolivia|      United States|   35|all the best|\n|            Anguilla|      United States|   19|all the best|\n|       United States|           Paraguay|    5|all the best|\n|       United States|        Afghanistan|    5|all the best|\n|Turks and Caicos ...|      United States|  183|all the best|\n|Saint Vincent and...|      United States|    6|all the best|\n|               Italy|      United States|  381|all the best|\n|            Pakistan|      United States|   12|all the best|\n|       United States|             Russia|  148|all the best|\n+--------------------+-------------------+-----+------------+\nonly showing top 20 rows\n\n+--------------------+-------------------+-----+------------+--------------+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|      wishes|value_count_10|\n+--------------------+-------------------+-----+------------+--------------+\n|       United States|            Croatia|    1|all the best|            -9|\n|       United States|            Ireland|  252|all the best|           242|\n|               Egypt|      United States|   13|all the best|             3|\n|       United States|              India|   62|all the best|            52|\n|       United States|          Singapore|   25|all the best|            15|\n|       United States|            Grenada|   46|all the best|            36|\n|          Costa Rica|      United States|  522|all the best|           512|\n|             Senegal|      United States|   31|all the best|            21|\n|              Guyana|      United States|   65|all the best|            55|\n|       United States|   Marshall Islands|   30|all the best|            20|\n|       United States|       Sint Maarten|  245|all the best|           235|\n|             Bolivia|      United States|   35|all the best|            25|\n|            Anguilla|      United States|   19|all the best|             9|\n|       United States|           Paraguay|    5|all the best|            -5|\n|       United States|        Afghanistan|    5|all the best|            -5|\n|Turks and Caicos ...|      United States|  183|all the best|           173|\n|Saint Vincent and...|      United States|    6|all the best|            -4|\n|               Italy|      United States|  381|all the best|           371|\n|            Pakistan|      United States|   12|all the best|             2|\n|       United States|             Russia|  148|all the best|           138|\n+--------------------+-------------------+-----+------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#column add in dataframe\n",
    "flightData3.withColumn(\"wishes\",lit(\"all the best\")).show()\n",
    "flightData3.withColumn(\"wishes\",lit(\"all the best\"))\\\n",
    ".withColumn(\"value_count_10\",col(\"count\")-10).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bf9c88f-9c45-4d30-abb9-df23e0dbd05f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count_renamed|\n+--------------------+-------------------+-------------+\n|       United States|            Croatia|            1|\n|       United States|            Ireland|          252|\n|               Egypt|      United States|           13|\n|       United States|              India|           62|\n|       United States|          Singapore|           25|\n|       United States|            Grenada|           46|\n|          Costa Rica|      United States|          522|\n|             Senegal|      United States|           31|\n|              Guyana|      United States|           65|\n|       United States|   Marshall Islands|           30|\n|       United States|       Sint Maarten|          245|\n|             Bolivia|      United States|           35|\n|            Anguilla|      United States|           19|\n|       United States|           Paraguay|            5|\n|       United States|        Afghanistan|            5|\n|Turks and Caicos ...|      United States|          183|\n|Saint Vincent and...|      United States|            6|\n|               Italy|      United States|          381|\n|            Pakistan|      United States|           12|\n|       United States|             Russia|          148|\n+--------------------+-------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#column rename in dataframe\n",
    "flightData3.withColumnRenamed(\"count\",\"count_renamed\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20a7322-59a8-46ba-b4a3-508db292844c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: string (nullable = true)\n\nroot\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# casting or changing data type of column in dataframe\n",
    "flightData3.withColumn(\"count\",col(\"count\").cast(\"string\")).printSchema()\n",
    "flightData3.withColumn(\"count\",col(\"count\").cast(\"long\")).printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7386da86-814d-4acd-899a-33f09b97b357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+--------------------+-------------------+\n|       United States|            Croatia|\n|       United States|            Ireland|\n|               Egypt|      United States|\n|       United States|              India|\n|       United States|          Singapore|\n|       United States|            Grenada|\n|          Costa Rica|      United States|\n|             Senegal|      United States|\n|              Guyana|      United States|\n|       United States|   Marshall Islands|\n|       United States|       Sint Maarten|\n|             Bolivia|      United States|\n|            Anguilla|      United States|\n|       United States|           Paraguay|\n|       United States|        Afghanistan|\n|Turks and Caicos ...|      United States|\n|Saint Vincent and...|      United States|\n|               Italy|      United States|\n|            Pakistan|      United States|\n|       United States|             Russia|\n+--------------------+-------------------+\nonly showing top 20 rows\n\n+-------------------+\n|ORIGIN_COUNTRY_NAME|\n+-------------------+\n|            Croatia|\n|            Ireland|\n|      United States|\n|              India|\n|          Singapore|\n|            Grenada|\n|      United States|\n|      United States|\n|      United States|\n|   Marshall Islands|\n|       Sint Maarten|\n|      United States|\n|      United States|\n|           Paraguay|\n|        Afghanistan|\n|      United States|\n|      United States|\n|      United States|\n|      United States|\n|             Russia|\n+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# column remove or dropping column from dataframe (column selecting we can use any method)\n",
    "flightData3.drop(\"count\",).show()\n",
    "flightData3.drop(\"DEST_COUNTRY_NAME\",col(\"count\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b429adfe-62f7-47da-960c-0880d5f4d6c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: 12"
     ]
    }
   ],
   "source": [
    "#union and union all\n",
    "data11=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(19 ,'Sohan',50000, 18)]\n",
    "\n",
    "data12=[(19 ,'Sohan',50000, 18),\n",
    "(20 ,'Sima',75000,  17)]\n",
    "schemam=[\"id\",\"name\",\"sal\",\"manager_id\"]\n",
    "\n",
    "data11df=spark.createDataFrame(data=data11,schema=schemam)\n",
    "#data11df.show()\n",
    "data12df=spark.createDataFrame(data=data12,schema=schemam)\n",
    "#data12df.show()\n",
    "\n",
    "#data11df.count()\n",
    "#data12df.count()\n",
    "\n",
    "data11df.union(data12df).count()\n",
    "#data11df.unionAll(data12df).count()\n",
    "\n",
    "#if we do union all through dataframe in spark then there is no difference between union and union all even for dublicates records union all will also result addtion of count in both dataframes \n",
    "#but in sql union will return unique rows only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f19df941-9533-4635-acc4-193376dd1fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-----+\n| id|  sal|manager_id| name|\n+---+-----+----------+-----+\n| 19|50000|        18|Sohan|\n| 20|75000|        17| Sima|\n+---+-----+----------+-----+\n\n+---+------+-----+----------+\n| id|  name|  sal|manager_id|\n+---+------+-----+----------+\n| 10|  Anil|50000|        18|\n| 11| Vikas|75000|        16|\n| 12| Nisha|40000|        18|\n| 13| Nidhi|60000|        17|\n| 14| Priya|80000|        18|\n| 15| Mohit|45000|        18|\n| 16|Rajesh|90000|        10|\n| 17| Raman|55000|        16|\n| 18|   Sam|65000|        17|\n| 19| Sohan|50000|        18|\n+---+------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "wrong_column_data=[(19 ,50000, 18,'Sohan'),\n",
    "(20 ,75000,  17,'Sima')]\n",
    "wrong_schema=[\"id\",\"sal\",\"manager_id\",\"name\"]\n",
    "wrong_column_datadf=spark.createDataFrame(data=wrong_column_data,schema=wrong_schema)\n",
    "wrong_column_datadf.show()\n",
    "data11df.show()\n",
    "data11df.union(wrong_column_datadf).show()\n",
    "#even sequnce of columns will be here and there it will place data wrongly but not fail \n",
    "# it will fail when number of columns will not match or column name will not match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86cd9452-9d5d-4e7d-81eb-5921904c5b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----------+\n| id|  name|  sal|manager_id|\n+---+------+-----+----------+\n| 10|  Anil|50000|        18|\n| 11| Vikas|75000|        16|\n| 12| Nisha|40000|        18|\n| 13| Nidhi|60000|        17|\n| 14| Priya|80000|        18|\n| 15| Mohit|45000|        18|\n| 16|Rajesh|90000|        10|\n| 17| Raman|55000|        16|\n| 18|   Sam|65000|        17|\n| 19| Sohan|50000|        18|\n| 19| Sohan|50000|        18|\n| 20|  Sima|75000|        17|\n+---+------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data11df.unionByName(wrong_column_datadf).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a5bee62-16f9-4a96-a503-bdf8726a2c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n|  id|   name| age|salary|country| department|\n+----+-------+----+------+-------+-----------+\n|   1| manish|  26| 20000|  india|         IT|\n|   2|  rahul|null| 40000|germany|engineering|\n|   3|  pawan|  12| 60000|  india|      sales|\n|   4|roshini|  44|  null|     uk|engineering|\n|   5|raushan|  35| 70000|  india|      sales|\n|   6|   null|  29|200000|     uk|         IT|\n|   7|   adam|  37| 65000|     us|         IT|\n|   8|  chris|  16| 40000|     us|      sales|\n|null|   null|null|  null|   null|       null|\n|   7|   adam|  37| 65000|     us|         IT|\n+----+-------+----+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# if else in spark\n",
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "emp_schema=[\"id\",\"name\",\"age\",\"salary\",\"country\",\"department\"]\n",
    "emp_df=spark.createDataFrame(data=emp_data,schema=emp_schema)\n",
    "empdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef13849f-b00f-4a1d-aa2a-3278803e60d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+--------+\n|  id|   name| age|salary|country| department|age_wise|\n+----+-------+----+------+-------+-----------+--------+\n|   1| manish|  26| 20000|  india|         IT|   major|\n|   2|  rahul|null| 40000|germany|engineering|    null|\n|   3|  pawan|  12| 60000|  india|      sales|   minor|\n|   4|roshini|  44|  null|     uk|engineering|   major|\n|   5|raushan|  35| 70000|  india|      sales|   major|\n|   6|   null|  29|200000|     uk|         IT|   major|\n|   7|   adam|  37| 65000|     us|         IT|   major|\n|   8|  chris|  16| 40000|     us|      sales|   minor|\n|null|   null|null|  null|   null|       null|    null|\n|   7|   adam|  37| 65000|     us|         IT|   major|\n+----+-------+----+------+-------+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "emp_df.withColumn(\"age_wise\", when (col(\"age\")>18, \"major\")\n",
    "                  .when(col(\"age\")<18, \"minor\").otherwise(\"null\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53f11b7-9552-48a5-bee6-6c3e5dced9d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+--------+\n|  id|   name| age|salary|country| department|age_wise|\n+----+-------+----+------+-------+-----------+--------+\n|   1| manish|  26| 20000|  india|         IT|   major|\n|   2|  rahul|null| 40000|germany|engineering|    null|\n|   3|  pawan|  12| 60000|  india|      sales|   minor|\n|   4|roshini|  44|  null|     uk|engineering|   major|\n|   5|raushan|  35| 70000|  india|      sales|   major|\n|   6|   null|  29|200000|     uk|         IT|   major|\n|   7|   adam|  37| 65000|     us|         IT|   major|\n|   8|  chris|  16| 40000|     us|      sales|   minor|\n|null|   null|null|  null|   null|       null|    null|\n|   7|   adam|  37| 65000|     us|         IT|   major|\n+----+-------+----+------+-------+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#by spark sql method\n",
    "emp_df.createOrReplaceTempView(\"empdata\")\n",
    "dfq=spark.sql(\"\"\"\n",
    "              select *,case when age> 18 then 'major'\n",
    "            when age<18 then 'minor'\n",
    "            else null end as age_wise from empdata \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e11d46-5a1e-46e3-986a-5d53d0020f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[47]: 12"
     ]
    }
   ],
   "source": [
    "#selecting distinct in spark\n",
    "data111=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(19 ,'Sohan',50000, 18),\n",
    "(19 ,'Sohan',50000, 18),\n",
    "(15 ,'Mohit',45000,  18)]\n",
    "\n",
    "schemamm=[\"id\",\"name\",\"sal\",\"manager_id\"]\n",
    "\n",
    "data111df=spark.createDataFrame(data=data111,schema=schemamm)\n",
    "data111df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9365829-7de1-4be3-bcf5-1f9a642f425b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[48]: 10"
     ]
    }
   ],
   "source": [
    "data111df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590be101-22ce-4396-8a29-a79509cf2442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----------+\n| id|  name|  sal|manager_id|\n+---+------+-----+----------+\n| 10|  Anil|50000|        18|\n| 12| Nisha|40000|        18|\n| 11| Vikas|75000|        16|\n| 13| Nidhi|60000|        17|\n| 15| Mohit|45000|        18|\n| 14| Priya|80000|        18|\n| 16|Rajesh|90000|        10|\n| 18|   Sam|65000|        17|\n| 17| Raman|55000|        16|\n| 19| Sohan|50000|        18|\n+---+------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data111df.select(\"id\",\"name\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06d90f13-baee-47e2-aa5e-41f8b571f6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----------+\n| id|  name|  sal|manager_id|\n+---+------+-----+----------+\n| 10|  Anil|50000|        18|\n| 12| Nisha|40000|        18|\n| 11| Vikas|75000|        16|\n| 13| Nidhi|60000|        17|\n| 15| Mohit|45000|        18|\n| 14| Priya|80000|        18|\n| 16|Rajesh|90000|        10|\n| 18|   Sam|65000|        17|\n| 17| Raman|55000|        16|\n| 19| Sohan|50000|        18|\n+---+------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data111df.dropDuplicates([\"id\",\"name\",\"sal\",\"manager_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da21aa4-c4e0-424b-8b02-789da6dcc735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----------+\n| id|  name|  sal|manager_id|\n+---+------+-----+----------+\n| 19| Sohan|50000|        18|\n| 19| Sohan|50000|        18|\n| 18|   Sam|65000|        17|\n| 17| Raman|55000|        16|\n| 16|Rajesh|90000|        10|\n| 15| Mohit|45000|        18|\n| 15| Mohit|45000|        18|\n| 14| Priya|80000|        18|\n| 13| Nidhi|60000|        17|\n| 12| Nisha|40000|        18|\n| 11| Vikas|75000|        16|\n| 10|  Anil|50000|        18|\n+---+------+-----+----------+\n\n+---+------+-----+----------+\n| id|  name|  sal|manager_id|\n+---+------+-----+----------+\n| 19| Sohan|50000|        18|\n| 19| Sohan|50000|        18|\n| 18|   Sam|65000|        17|\n| 17| Raman|55000|        16|\n| 16|Rajesh|90000|        10|\n| 15| Mohit|45000|        18|\n| 15| Mohit|45000|        18|\n| 14| Priya|80000|        18|\n| 13| Nidhi|60000|        17|\n| 12| Nisha|40000|        18|\n| 11| Vikas|75000|        16|\n| 10|  Anil|50000|        18|\n+---+------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# sorting data in asc and desc in spark\n",
    "\n",
    "data111df.sort(col(\"id\").desc()).show()\n",
    "data111df.sort(col(\"id\").desc(),col(\"name\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34b4461-ffb1-4b79-868d-b18aa0c6e62c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n|  id|   name| age|salary|country| department|\n+----+-------+----+------+-------+-----------+\n|   1| manish|  26| 20000|  india|         IT|\n|   2|  rahul|null| 40000|germany|engineering|\n|   3|  pawan|  12| 60000|  india|      sales|\n|   4|roshini|  44|  null|     uk|engineering|\n|   5|raushan|  35| 70000|  india|      sales|\n|   6|   null|  29|200000|     uk|         IT|\n|   7|   adam|  37| 65000|     us|         IT|\n|   8|  chris|  16| 40000|     us|      sales|\n|null|   null|null|  null|   null|       null|\n|   7|   adam|  37| 65000|     us|         IT|\n+----+-------+----+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#aggregation in spark\n",
    "# if else in spark\n",
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "emp_schema=[\"id\",\"name\",\"age\",\"salary\",\"country\",\"department\"]\n",
    "emp_df=spark.createDataFrame(data=emp_data,schema=emp_schema)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d414ec1-ca02-4920-9c6e-01de56002548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 10"
     ]
    }
   ],
   "source": [
    "emp_df.count()\n",
    "#here count is action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8567e012-2d6b-4c6a-a61f-34c0cefc6e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: DataFrame[count(name): bigint]"
     ]
    }
   ],
   "source": [
    "emp_df.select(count(\"name\"))\n",
    "# here count is transformation only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44999c99-e786-45b2-a78e-f7a8923f5391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|count(name)|\n+-----------+\n|          8|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count(\"name\")).show()\n",
    "# if we run count on one colum it skips null value present in that colum  and gives count while doing count * or 1 it did not skips null rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1c1a5d-df91-4e8f-9f59-397fa8e66f16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|count(name)|\n+-----------+\n|          8|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count(col(\"name\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "259f3afa-aa66-4c1a-af2a-f641495ce623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-----------+\n|count(name)|min(salary)|max(salary)|avg(salary)|\n+-----------+-----------+-----------+-----------+\n|          8|      20000|     200000|    70000.0|\n+-----------+-----------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count(\"name\"),min(\"salary\"),max(\"salary\"),avg(\"salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55785552-97b9-406f-8c37-31f130f76f19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+\n|count_name|min_salary|max_salary|avg_salary|\n+----------+----------+----------+----------+\n|         8|     20000|    200000|   70000.0|\n+----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(count(\"name\").alias(\"count_name\"),min(\"salary\").alias(\"min_salary\"),max(\"salary\").alias(\"max_salary\"),avg(\"salary\").alias(\"avg_salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "186f15f7-7a31-4851-9978-ef68b3c43bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n| department|sum(salary)|\n+-----------+-----------+\n|         IT|     350000|\n|engineering|      40000|\n|      sales|     170000|\n|       null|       null|\n+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#group by in spark\n",
    "emp_df.groupBy(\"department\")\\\n",
    "    .agg(sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da121979-8311-45af-868f-9f739b72f01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-----------+-----------+\n| department|country|sum(salary)|count(name)|\n+-----------+-------+-----------+-----------+\n|         IT|  india|      20000|          1|\n|engineering|germany|      40000|          1|\n|      sales|  india|     130000|          2|\n|engineering|     uk|       null|          1|\n|         IT|     uk|     200000|          0|\n|         IT|     us|     130000|          2|\n|      sales|     us|      40000|          1|\n|       null|   null|       null|          0|\n+-----------+-------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#group by in spark\n",
    "emp_df.groupBy(\"department\",\"country\")\\\n",
    "    .agg(sum(\"salary\"),count(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0277317e-cc29-4c5b-ada3-2d5e70391fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          1|       manish|  patna|     30-05-2022|\n|          2|       vikash|kolkata|     12-03-2023|\n|          3|       nikita|  delhi|     25-06-2023|\n|          4|        rahul| ranchi|     24-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|\n|          7|        raman|  patna|     30-12-2022|\n|          8|      prakash| ranchi|     24-02-2023|\n|          9|       ragini|kolkata|     03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|\n+-----------+-------------+-------+---------------+\n\n+-----------+----------+--------+----------------+\n|customer_id|product_id|quantity|date_of_purchase|\n+-----------+----------+--------+----------------+\n|          1|        22|      10|      01-06-2022|\n|          1|        27|       5|      03-02-2023|\n|          2|         5|       3|      01-06-2023|\n|          5|        22|       1|      22-03-2023|\n|          7|        22|       4|      03-02-2023|\n|          9|         5|       6|      03-03-2023|\n|          2|         1|      12|      15-06-2023|\n|          1|        56|       2|      25-06-2023|\n|          5|        12|       5|      15-04-2023|\n|         11|        12|      76|      12-03-2023|\n+-----------+----------+--------+----------------+\n\n+---+-------+-----+\n| id|   name|price|\n+---+-------+-----+\n|  1|  fanta|   20|\n|  2|    dew|   22|\n|  5| sprite|   40|\n|  7|redbull|  100|\n| 12|  mazza|   45|\n| 22|   coke|   27|\n| 25|  limca|   21|\n| 27|  pepsi|   14|\n| 56|  sting|   10|\n+---+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#joins\n",
    "\n",
    "customer_data = [(1,'manish','patna',\"30-05-2022\"),\n",
    "(2,'vikash','kolkata',\"12-03-2023\"),\n",
    "(3,'nikita','delhi',\"25-06-2023\"),\n",
    "(4,'rahul','ranchi',\"24-03-2023\"),\n",
    "(5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "(6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "(7,'raman','patna',\"30-12-2022\"),\n",
    "(8,'prakash','ranchi',\"24-02-2023\"),\n",
    "(9,'ragini','kolkata',\"03-03-2023\"),\n",
    "(10,'raushan','jaipur',\"05-02-2023\")]\n",
    "\n",
    "customer_schema=['customer_id','customer_name','address','date_of_joining']\n",
    "\n",
    "\n",
    "sales_data = [(1,22,10,\"01-06-2022\"),\n",
    "(1,27,5,\"03-02-2023\"),\n",
    "(2,5,3,\"01-06-2023\"),\n",
    "(5,22,1,\"22-03-2023\"),\n",
    "(7,22,4,\"03-02-2023\"),\n",
    "(9,5,6,\"03-03-2023\"),\n",
    "(2,1,12,\"15-06-2023\"),\n",
    "(1,56,2,\"25-06-2023\"),\n",
    "(5,12,5,\"15-04-2023\"),\n",
    "(11,12,76,\"12-03-2023\")]\n",
    "\n",
    "sales_schema=['customer_id','product_id','quantity','date_of_purchase']\n",
    "\n",
    "\n",
    "product_data = [(1, 'fanta',20),\n",
    "(2, 'dew',22),\n",
    "(5, 'sprite',40),\n",
    "(7, 'redbull',100),\n",
    "(12,'mazza',45),\n",
    "(22,'coke',27),\n",
    "(25,'limca',21),\n",
    "(27,'pepsi',14),\n",
    "(56,'sting',10)]\n",
    "\n",
    "product_schema=['id','name','price']\n",
    "\n",
    "customer_df=spark.createDataFrame(data=customer_data,schema=customer_schema)\n",
    "sales_df=spark.createDataFrame(data=sales_data,schema=sales_schema)\n",
    "product_df=spark.createDataFrame(data=product_data,schema=product_schema)\n",
    "customer_df.show()\n",
    "sales_df.show()\n",
    "product_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18835dd5-1993-42ae-8890-744ff4e44941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd087dcb-1b3a-4a73-8db1-03957703f331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.join(df2,df2[\"joined column\"]==df1[\"joined column\"],\"jointype\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0c318c-a48a-4bba-ae4d-78f386478cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# inner join the bas of one column \n",
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61387eaa-56a2-4b32-8e0f-db77eafda311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-1780351459353938>\", line 2, in <module>\n    customer_df.join(sales_df,(sales_df[\"customer_id\"]==customer_df[\"customer_id\"]) & (sales_df[\"product_id\"]==customer_df[\"product_id\"]),\"inner\").select(sales_df[\"product_id\"]).sort(\"product_id\").show()\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/dataframe.py\", line 2918, in __getitem__\n    jc = self._jdf.apply(item)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/errors/exceptions.py\", line 234, in deco\n    raise converted from None\npyspark.errors.exceptions.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_name`, `address`, `date_of_joining`].\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `product_id` cannot be resolved. Did you mean one of the following? [`customer_id`, `customer_name`, `address`, `date_of_joining`].",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inner join the bas of more than column \n",
    "customer_df.join(sales_df,(sales_df[\"customer_id\"]==customer_df[\"customer_id\"]) & (sales_df[\"product_id\"]==customer_df[\"product_id\"]),\"inner\").select(sales_df[\"product_id\"]).sort(\"product_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af27a944-386b-4768-9089-b52e124a9638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# left join the bas of one column \n",
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222e1446-1b3a-4713-b471-48624018ba21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|       null|         null|   null|           null|         11|        12|      76|      12-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# right join the bas of one column \n",
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584ad08c-6657-45f1-9bec-fd8b1cbf4826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n|       null|         null|   null|           null|         11|        12|      76|      12-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# outer join the bas of one column \n",
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c874c97b-b713-41be-a64f-d96e505859ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          1|       manish|  patna|     30-05-2022|\n|          2|       vikash|kolkata|     12-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|\n|          7|        raman|  patna|     30-12-2022|\n|          9|       ragini|kolkata|     03-03-2023|\n+-----------+-------------+-------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# left_semi join the bas of one column \n",
    "# pick only left table data matching only\n",
    "customer_df.join(sales_df,sales_df[\"customer_id\"]==customer_df[\"customer_id\"],\"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb153d1-2229-4585-90cf-4d1ee985857b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          2|         5|       3|      01-06-2023|\n|          1|       manish|  patna|     30-05-2022|          5|        22|       1|      22-03-2023|\n|          1|       manish|  patna|     30-05-2022|          7|        22|       4|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          9|         5|       6|      03-03-2023|\n|          1|       manish|  patna|     30-05-2022|          2|         1|      12|      15-06-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          1|       manish|  patna|     30-05-2022|          5|        12|       5|      15-04-2023|\n|          1|       manish|  patna|     30-05-2022|         11|        12|      76|      12-03-2023|\n|          2|       vikash|kolkata|     12-03-2023|          1|        22|      10|      01-06-2022|\n|          2|       vikash|kolkata|     12-03-2023|          1|        27|       5|      03-02-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          5|        22|       1|      22-03-2023|\n|          2|       vikash|kolkata|     12-03-2023|          7|        22|       4|      03-02-2023|\n|          2|       vikash|kolkata|     12-03-2023|          9|         5|       6|      03-03-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          5|        12|       5|      15-04-2023|\n|          2|       vikash|kolkata|     12-03-2023|         11|        12|      76|      12-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.crossJoin(sales_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08dfa8ca-ecc5-4661-a184-dea3e08fb5f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+\n| id|    name|salary|     dept|gender|\n+---+--------+------+---------+------+\n|  1|  manish| 50000|       IT|     m|\n|  2|  vikash| 60000|    sales|     m|\n|  3| raushan| 70000|marketing|     m|\n|  4|  mukesh| 80000|       IT|     m|\n|  5|   priti| 90000|    sales|     f|\n|  6|  nikita| 45000|marketing|     f|\n|  7|  ragini| 55000|marketing|     f|\n|  8|   rashi|100000|       IT|     f|\n|  9|  aditya| 65000|       IT|     m|\n| 10|   rahul| 50000|marketing|     m|\n| 11|   rakhi| 50000|       IT|     f|\n| 12|akhilesh| 90000|    sales|     m|\n+---+--------+------+---------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "emp_data = [(1,'manish',50000,'IT','m'),\n",
    "(2,'vikash',60000,'sales','m'),\n",
    "(3,'raushan',70000,'marketing','m'),\n",
    "(4,'mukesh',80000,'IT','m'),\n",
    "(5,'priti',90000,'sales','f'),\n",
    "(6,'nikita',45000,'marketing','f'),\n",
    "(7,'ragini',55000,'marketing','f'),\n",
    "(8,'rashi',100000,'IT','f'),\n",
    "(9,'aditya',65000,'IT','m'),\n",
    "(10,'rahul',50000,'marketing','m'),\n",
    "(11,'rakhi',50000,'IT','f'),\n",
    "(12,'akhilesh',90000,'sales','m')]\n",
    "\n",
    "emp_schema=[\"id\",\"name\",\"salary\",\"dept\",\"gender\"]\n",
    "emp_df=spark.createDataFrame(data=emp_data,schema=emp_schema)\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed1d18f0-0726-4325-b1cd-180cbd037022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d0898e-b184-4230-84ed-0790e633c20a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n|     dept|sum(salary)|\n+---------+-----------+\n|       IT|     345000|\n|marketing|     220000|\n|    sales|     240000|\n+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# group by will give only those columns which are there in grouping with aggregation result\n",
    "#  what if we need all the columns present in the tables in select statement the either we need to use window function or this group by result again need to join with original table and take all colummns from there \n",
    "emp_df.groupBy(\"dept\").agg(sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be9a030b-d280-4753-b3cb-d7c3f227165b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26cfd269-c40b-4dda-8663-600e9d704120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2697387482554554>:4\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\u001B[1;32m      3\u001B[0m window\u001B[38;5;241m=\u001B[39mWindow\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 4\u001B[0m emp_df\u001B[38;5;241m=\u001B[39memp_df\u001B[38;5;241m.\u001B[39mwithcolumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumnname\u001B[39m\u001B[38;5;124m\"\u001B[39m,aggregation)\u001B[38;5;241m.\u001B[39mover(window)\n",
       "\u001B[1;32m      5\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mshow\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2964\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n",
       "\u001B[1;32m   2934\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n",
       "\u001B[1;32m   2935\u001B[0m \n",
       "\u001B[1;32m   2936\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2961\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n",
       "\u001B[1;32m   2962\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   2963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n",
       "\u001B[0;32m-> 2964\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n",
       "\u001B[1;32m   2965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n",
       "\u001B[1;32m   2966\u001B[0m     )\n",
       "\u001B[1;32m   2967\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n",
       "\u001B[1;32m   2968\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'withcolumn'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-2697387482554554>:4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m      3\u001B[0m window\u001B[38;5;241m=\u001B[39mWindow\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m emp_df\u001B[38;5;241m=\u001B[39memp_df\u001B[38;5;241m.\u001B[39mwithcolumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumnname\u001B[39m\u001B[38;5;124m\"\u001B[39m,aggregation)\u001B[38;5;241m.\u001B[39mover(window)\n\u001B[1;32m      5\u001B[0m emp_df\u001B[38;5;241m.\u001B[39mshow\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2964\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   2934\u001B[0m \u001B[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001B[39;00m\n\u001B[1;32m   2935\u001B[0m \n\u001B[1;32m   2936\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2961\u001B[0m \u001B[38;5;124;03m+---+\u001B[39;00m\n\u001B[1;32m   2962\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m-> 2964\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m   2965\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n\u001B[1;32m   2966\u001B[0m     )\n\u001B[1;32m   2967\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mapply(name)\n\u001B[1;32m   2968\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n\n\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'withcolumn'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DataFrame' object has no attribute 'withcolumn'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# group by will give only those columns which are there in grouping and then the aggregation what if we need all the columns present in the tables in select statement the either we need to use window function or this group by result again need to join with original table and take all colummns from there \n",
    "\n",
    "# window function provide a seperate window on the basis for key given for that window or that partiotion key now on that window we can perform aggregations on the basis od sum column and add that value in a addition column for each row \n",
    "from pyspark.sql.window import *\n",
    "window=Window.partitionBy(\"col\").orderBy(\"col\")\n",
    "your_df=yourdf.withColumn(\"total_sal\",sum(\"salary\")).over(window)\n",
    "emp_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08dad0a-2f93-4542-8cba-deb3cc5118f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+---------+\n| id|    name|salary|     dept|gender|total_sal|\n+---+--------+------+---------+------+---------+\n|  1|  manish| 50000|       IT|     m|   345000|\n|  4|  mukesh| 80000|       IT|     m|   345000|\n|  8|   rashi|100000|       IT|     f|   345000|\n|  9|  aditya| 65000|       IT|     m|   345000|\n| 11|   rakhi| 50000|       IT|     f|   345000|\n|  3| raushan| 70000|marketing|     m|   220000|\n|  6|  nikita| 45000|marketing|     f|   220000|\n|  7|  ragini| 55000|marketing|     f|   220000|\n| 10|   rahul| 50000|marketing|     m|   220000|\n|  2|  vikash| 60000|    sales|     m|   240000|\n|  5|   priti| 90000|    sales|     f|   240000|\n| 12|akhilesh| 90000|    sales|     m|   240000|\n+---+--------+------+---------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "window=Window.partitionBy(\"dept\")\n",
    "emp_df=emp_df.withColumn(\"total_sal\",sum(col(\"salary\")).over(window))\n",
    "emp_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1b86b1e-59ac-4e36-a78f-1e17e3fc5f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# row number provides a unique value (rank) for each raw \n",
    "# rank provide rank for each row and if some value on the basis of which order by is happening it will dublictae then rank and provide a gap for next rank \n",
    "# dense rank provide a rank for each row if some value on the basis of which order by is happening it will dublictae then dense dublicate the rank and next rank will start just after previous rank no gap for dublicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e372c39-85ed-44ff-817d-5bd00c3f5669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+----------+----+----------+\n| id|    name|salary|     dept|gender|Row_Number|Rank|Dense_Rank|\n+---+--------+------+---------+------+----------+----+----------+\n|  1|  manish| 50000|       IT|     m|         1|   1|         1|\n| 11|   rakhi| 50000|       IT|     f|         2|   1|         1|\n|  9|  aditya| 65000|       IT|     m|         3|   3|         2|\n|  4|  mukesh| 80000|       IT|     m|         4|   4|         3|\n|  8|   rashi|100000|       IT|     f|         5|   5|         4|\n|  6|  nikita| 45000|marketing|     f|         1|   1|         1|\n| 10|   rahul| 50000|marketing|     m|         2|   2|         2|\n|  7|  ragini| 55000|marketing|     f|         3|   3|         3|\n|  3| raushan| 70000|marketing|     m|         4|   4|         4|\n|  2|  vikash| 60000|    sales|     m|         1|   1|         1|\n|  5|   priti| 90000|    sales|     f|         2|   2|         2|\n| 12|akhilesh| 90000|    sales|     m|         3|   2|         2|\n+---+--------+------+---------+------+----------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "window=Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "df1=emp_df.withColumn(\"Row_Number\",row_number().over(window))\\\n",
    "    .withColumn(\"Rank\",rank().over(window))\\\n",
    "    .withColumn(\"Dense_Rank\",dense_rank().over(window))\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc43b80-6dba-4665-99e1-cfeb65300a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+----------+----+----------+\n| id|    name|salary|     dept|gender|Row_Number|Rank|Dense_Rank|\n+---+--------+------+---------+------+----------+----+----------+\n| 11|   rakhi| 50000|       IT|     f|         1|   1|         1|\n|  8|   rashi|100000|       IT|     f|         2|   2|         2|\n|  1|  manish| 50000|       IT|     m|         1|   1|         1|\n|  9|  aditya| 65000|       IT|     m|         2|   2|         2|\n|  4|  mukesh| 80000|       IT|     m|         3|   3|         3|\n|  6|  nikita| 45000|marketing|     f|         1|   1|         1|\n|  7|  ragini| 55000|marketing|     f|         2|   2|         2|\n| 10|   rahul| 50000|marketing|     m|         1|   1|         1|\n|  3| raushan| 70000|marketing|     m|         2|   2|         2|\n|  5|   priti| 90000|    sales|     f|         1|   1|         1|\n|  2|  vikash| 60000|    sales|     m|         1|   1|         1|\n| 12|akhilesh| 90000|    sales|     m|         2|   2|         2|\n+---+--------+------+---------+------+----------+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "window=Window.partitionBy(\"dept\",\"gender\").orderBy(\"salary\")\n",
    "new_df=emp_df.withColumn(\"Row_Number\",row_number().over(window))\\\n",
    "    .withColumn(\"Rank\",rank().over(window))\\\n",
    "    .withColumn(\"Dense_Rank\",dense_rank().over(window))\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b097d662-3320-4935-902c-2a968592b507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+----------+\n| id|    name|salary|     dept|gender|Dense_Rank|\n+---+--------+------+---------+------+----------+\n|  8|   rashi|100000|       IT|     f|         1|\n|  4|  mukesh| 80000|       IT|     m|         2|\n|  3| raushan| 70000|marketing|     m|         1|\n|  7|  ragini| 55000|marketing|     f|         2|\n|  5|   priti| 90000|    sales|     f|         1|\n| 12|akhilesh| 90000|    sales|     m|         1|\n|  2|  vikash| 60000|    sales|     m|         2|\n+---+--------+------+---------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# top two employees from each department \n",
    "from pyspark.sql.window import *\n",
    "emp_data = [(1,'manish',50000,'IT','m'),\n",
    "(2,'vikash',60000,'sales','m'),\n",
    "(3,'raushan',70000,'marketing','m'),\n",
    "(4,'mukesh',80000,'IT','m'),\n",
    "(5,'priti',90000,'sales','f'),\n",
    "(6,'nikita',45000,'marketing','f'),\n",
    "(7,'ragini',55000,'marketing','f'),\n",
    "(8,'rashi',100000,'IT','f'),\n",
    "(9,'aditya',65000,'IT','m'),\n",
    "(10,'rahul',50000,'marketing','m'),\n",
    "(11,'rakhi',50000,'IT','f'),\n",
    "(12,'akhilesh',90000,'sales','m')]\n",
    "\n",
    "emp_schema=[\"id\",\"name\",\"salary\",\"dept\",\"gender\"]\n",
    "emp_df=spark.createDataFrame(data=emp_data,schema=emp_schema)\n",
    "window=Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "top_df=emp_df.withColumn(\"Dense_Rank\",dense_rank().over(window)).filter(col(\"Dense_Rank\")<=2)\n",
    "top_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3a79290-338b-430e-a21b-90dae0bf79eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---------+------+\n| id|    name|salary|     dept|gender|\n+---+--------+------+---------+------+\n|  1|  manish| 50000|       IT|     m|\n|  2|  vikash| 60000|    sales|     m|\n|  3| raushan| 70000|marketing|     m|\n|  4|  mukesh| 80000|       IT|     m|\n|  5|   priti| 90000|    sales|     f|\n|  6|  nikita| 45000|marketing|     f|\n|  7|  ragini| 55000|marketing|     f|\n|  8|   rashi|100000|       IT|     f|\n|  9|  aditya| 65000|       IT|     m|\n| 10|   rahul| 50000|marketing|     m|\n| 11|   rakhi| 50000|       IT|     f|\n| 12|akhilesh| 90000|    sales|     m|\n+---+--------+------+---------+------+\n\n+----------+------------+----------+-------+\n|product_id|product_name|sales_date|  sales|\n+----------+------------+----------+-------+\n|         1|      iphone|01-01-2023|1500000|\n|         2|     samsung|01-01-2023|1100000|\n|         3|     oneplus|01-01-2023|1100000|\n|         1|      iphone|01-02-2023|1300000|\n|         2|     samsung|01-02-2023|1120000|\n|         3|     oneplus|01-02-2023|1120000|\n|         1|      iphone|01-03-2023|1600000|\n|         2|     samsung|01-03-2023|1080000|\n|         3|     oneplus|01-03-2023|1160000|\n|         1|      iphone|01-04-2023|1700000|\n|         2|     samsung|01-04-2023|1800000|\n|         3|     oneplus|01-04-2023|1170000|\n|         1|      iphone|01-05-2023|1200000|\n|         2|     samsung|01-05-2023| 980000|\n|         3|     oneplus|01-05-2023|1175000|\n|         1|      iphone|01-06-2023|1100000|\n|         2|     samsung|01-06-2023|1100000|\n|         3|     oneplus|01-06-2023|1200000|\n+----------+------------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "product_data = [\n",
    "(1,\"iphone\",\"01-01-2023\",1500000),\n",
    "(2,\"samsung\",\"01-01-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "product_schema=['product_id','product_name','sales_date','sales']\n",
    "\n",
    "\n",
    "emp_data = [(1,'manish',50000,'IT','m'),\n",
    "(2,'vikash',60000,'sales','m'),\n",
    "(3,'raushan',70000,'marketing','m'),\n",
    "(4,'mukesh',80000,'IT','m'),\n",
    "(5,'priti',90000,'sales','f'),\n",
    "(6,'nikita',45000,'marketing','f'),\n",
    "(7,'ragini',55000,'marketing','f'),\n",
    "(8,'rashi',100000,'IT','f'),\n",
    "(9,'aditya',65000,'IT','m'),\n",
    "(10,'rahul',50000,'marketing','m'),\n",
    "(11,'rakhi',50000,'IT','f'),\n",
    "(12,'akhilesh',90000,'sales','m')]\n",
    "\n",
    "emp_schema=['id','name','salary','dept','gender']\n",
    "product_df=spark.createDataFrame(data=product_data,schema=product_schema)\n",
    "emp_df = spark.createDataFrame(data=emp_data,schema=emp_schema)\n",
    "emp_df.show()\n",
    "product_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79bf90a4-38d6-4284-9406-8bddf03dd24a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+----------------+\n|product_id|product_name|sales_date|  sales|last_month_sales|\n+----------+------------+----------+-------+----------------+\n|         1|      iphone|01-01-2023|1500000|            null|\n|         1|      iphone|01-02-2023|1300000|         1500000|\n|         1|      iphone|01-03-2023|1600000|         1300000|\n|         1|      iphone|01-04-2023|1700000|         1600000|\n|         1|      iphone|01-05-2023|1200000|         1700000|\n|         1|      iphone|01-06-2023|1100000|         1200000|\n|         3|     oneplus|01-01-2023|1100000|            null|\n|         3|     oneplus|01-02-2023|1120000|         1100000|\n|         3|     oneplus|01-03-2023|1160000|         1120000|\n|         3|     oneplus|01-04-2023|1170000|         1160000|\n|         3|     oneplus|01-05-2023|1175000|         1170000|\n|         3|     oneplus|01-06-2023|1200000|         1175000|\n|         2|     samsung|01-01-2023|1100000|            null|\n|         2|     samsung|01-02-2023|1120000|         1100000|\n|         2|     samsung|01-03-2023|1080000|         1120000|\n|         2|     samsung|01-04-2023|1800000|         1080000|\n|         2|     samsung|01-05-2023| 980000|         1800000|\n|         2|     samsung|01-06-2023|1100000|          980000|\n+----------+------------+----------+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "window=Window.partitionBy(\"product_name\").orderBy(\"sales_date\")\n",
    "last_month_df=product_df.withColumn(\"last_month_sales\",lag(col(\"sales\"),1).over(window))\n",
    "last_month_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca7fef01-e23c-41a5-ba13-570cade6c108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+----------------+-------------------------+\n|product_id|product_name|sales_date|  sales|last_month_sales|profit_or_loss_percentage|\n+----------+------------+----------+-------+----------------+-------------------------+\n|         1|      iphone|01-01-2023|1500000|            null|                     null|\n|         1|      iphone|01-02-2023|1300000|         1500000|      -15.384615384615385|\n|         1|      iphone|01-03-2023|1600000|         1300000|                    18.75|\n|         1|      iphone|01-04-2023|1700000|         1600000|         5.88235294117647|\n|         1|      iphone|01-05-2023|1200000|         1700000|       -41.66666666666667|\n|         1|      iphone|01-06-2023|1100000|         1200000|       -9.090909090909092|\n|         3|     oneplus|01-01-2023|1100000|            null|                     null|\n|         3|     oneplus|01-02-2023|1120000|         1100000|       1.7857142857142856|\n|         3|     oneplus|01-03-2023|1160000|         1120000|       3.4482758620689653|\n|         3|     oneplus|01-04-2023|1170000|         1160000|       0.8547008547008548|\n|         3|     oneplus|01-05-2023|1175000|         1170000|        0.425531914893617|\n|         3|     oneplus|01-06-2023|1200000|         1175000|        2.083333333333333|\n|         2|     samsung|01-01-2023|1100000|            null|                     null|\n|         2|     samsung|01-02-2023|1120000|         1100000|       1.7857142857142856|\n|         2|     samsung|01-03-2023|1080000|         1120000|      -3.7037037037037033|\n|         2|     samsung|01-04-2023|1800000|         1080000|                     40.0|\n|         2|     samsung|01-05-2023| 980000|         1800000|        -83.6734693877551|\n|         2|     samsung|01-06-2023|1100000|          980000|       10.909090909090908|\n+----------+------------+----------+-------+----------------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "p_df=last_month_df.withColumn(\"profit_or_loss_percentage\",((col(\"sales\")-col(\"last_month_sales\"))/col(\"sales\")*100))\n",
    "p_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6def359-0b2d-44a7-801f-faa1913c53db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+----------------+-------------------------+\n|product_id|product_name|sales_date|  sales|last_month_sales|profit_or_loss_percentage|\n+----------+------------+----------+-------+----------------+-------------------------+\n|         1|      iphone|01-01-2023|1500000|            null|                     null|\n|         1|      iphone|01-02-2023|1300000|         1500000|      -15.384615384615385|\n|         1|      iphone|01-03-2023|1600000|         1300000|                    18.75|\n|         1|      iphone|01-04-2023|1700000|         1600000|         5.88235294117647|\n|         1|      iphone|01-05-2023|1200000|         1700000|       -41.66666666666667|\n|         1|      iphone|01-06-2023|1100000|         1200000|       -9.090909090909092|\n|         3|     oneplus|01-01-2023|1100000|            null|                     null|\n|         3|     oneplus|01-02-2023|1120000|         1100000|       1.7857142857142856|\n|         3|     oneplus|01-03-2023|1160000|         1120000|       3.4482758620689653|\n|         3|     oneplus|01-04-2023|1170000|         1160000|       0.8547008547008548|\n|         3|     oneplus|01-05-2023|1175000|         1170000|        0.425531914893617|\n|         3|     oneplus|01-06-2023|1200000|         1175000|        2.083333333333333|\n|         2|     samsung|01-01-2023|1100000|            null|                     null|\n|         2|     samsung|01-02-2023|1120000|         1100000|       1.7857142857142856|\n|         2|     samsung|01-03-2023|1080000|         1120000|      -3.7037037037037033|\n|         2|     samsung|01-04-2023|1800000|         1080000|                     40.0|\n|         2|     samsung|01-05-2023| 980000|         1800000|        -83.6734693877551|\n|         2|     samsung|01-06-2023|1100000|          980000|       10.909090909090908|\n+----------+------------+----------+-------+----------------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "p_df=last_month_df.withColumn(\"profit_or_loss_percentage\",((col(\"sales\")-col(\"last_month_sales\"))/col(\"sales\")*100))\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b677b95a-0fa9-4a8d-b5ca-f691ad2471a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+----------+\n|product_id|product_name|sales_date|  sales|firstvalue|\n+----------+------------+----------+-------+----------+\n|         1|      iphone|01-01-2023|1500000|   1500000|\n|         1|      iphone|01-02-2023|1300000|   1500000|\n|         1|      iphone|01-03-2023|1600000|   1500000|\n|         1|      iphone|01-04-2023|1700000|   1500000|\n|         1|      iphone|01-05-2023|1200000|   1500000|\n|         1|      iphone|01-06-2023|1100000|   1500000|\n|         3|     oneplus|01-01-2023|1100000|   1100000|\n|         3|     oneplus|01-02-2023|1120000|   1100000|\n|         3|     oneplus|01-03-2023|1160000|   1100000|\n|         3|     oneplus|01-04-2023|1170000|   1100000|\n|         3|     oneplus|01-05-2023|1175000|   1100000|\n|         3|     oneplus|01-06-2023|1200000|   1100000|\n|         2|     samsung|01-01-2023|1100000|   1100000|\n|         2|     samsung|01-02-2023|1120000|   1100000|\n|         2|     samsung|01-03-2023|1080000|   1100000|\n|         2|     samsung|01-04-2023|1800000|   1100000|\n|         2|     samsung|01-05-2023| 980000|   1100000|\n|         2|     samsung|01-06-2023|1100000|   1100000|\n+----------+------------+----------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "product_data = [\n",
    "(1,\"iphone\",\"01-01-2023\",1500000),\n",
    "(2,\"samsung\",\"01-01-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "product_schema=['product_id','product_name','sales_date','sales']\n",
    "\n",
    "\n",
    "product_df=spark.createDataFrame(data=product_data,schema=product_schema)\n",
    "window=Window.partitionBy(\"product_name\").orderBy(\"sales_date\")\n",
    "fisrst_df=product_df.withColumn(\"firstvalue\",first(\"sales\").over(window))\n",
    "fisrst_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c57ac0e-f94a-4c12-a906-64f38b021e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+----------+---------+\n|product_id|product_name|sales_date|  sales|firstvalue|lastvalue|\n+----------+------------+----------+-------+----------+---------+\n|         1|      iphone|01-01-2023|1500000|   1500000|  1100000|\n|         1|      iphone|01-02-2023|1300000|   1500000|  1100000|\n|         1|      iphone|01-03-2023|1600000|   1500000|  1100000|\n|         1|      iphone|01-04-2023|1700000|   1500000|  1100000|\n|         1|      iphone|01-05-2023|1200000|   1500000|  1100000|\n|         1|      iphone|01-06-2023|1100000|   1500000|  1100000|\n|         3|     oneplus|01-01-2023|1100000|   1100000|  1200000|\n|         3|     oneplus|01-02-2023|1120000|   1100000|  1200000|\n|         3|     oneplus|01-03-2023|1160000|   1100000|  1200000|\n|         3|     oneplus|01-04-2023|1170000|   1100000|  1200000|\n|         3|     oneplus|01-05-2023|1175000|   1100000|  1200000|\n|         3|     oneplus|01-06-2023|1200000|   1100000|  1200000|\n|         2|     samsung|01-01-2023|1100000|   1100000|  1100000|\n|         2|     samsung|01-02-2023|1120000|   1100000|  1100000|\n|         2|     samsung|01-03-2023|1080000|   1100000|  1100000|\n|         2|     samsung|01-04-2023|1800000|   1100000|  1100000|\n|         2|     samsung|01-05-2023| 980000|   1100000|  1100000|\n|         2|     samsung|01-06-2023|1100000|   1100000|  1100000|\n+----------+------------+----------+-------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "window=Window.partitionBy(\"product_name\").orderBy(\"sales_date\").rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "fisrst_df=product_df.withColumn(\"firstvalue\",first(\"sales\").over(window))\\\n",
    "    .withColumn(\"lastvalue\",last(\"sales\").over(window))\n",
    "fisrst_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14de0a45-4d0e-4f37-8e70-eec1be270dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+-------------+-------------+-------------+------+\n|code|message|         restaurants|results_found|results_shown|results_start|status|\n+----+-------+--------------------+-------------+-------------+-------------+------+\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17066603}, b9...|         6835|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17093124}, b9...|         8680|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17580142}, b9...|          943|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17284158}, b9...|          257|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17678233}, b9...|          358|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17375047}, b9...|          641|           20|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|                  []|            0|            0|            1|  null|\n|null|   null|[{{{17616590}, b9...|         1613|           20|            1|  null|\n+----+-------+--------------------+-------------+-------------+-------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#json flattning\n",
    "jsonData = spark.read.format(\"json\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"mode\",\"permissive\")\\\n",
    ".option(\"inferschema\",\"true\")\\\n",
    ".option(\"multiline\",\"true\")\\\n",
    ".load(\"/FileStore/tables/resturant_json_data.json\")\n",
    "\n",
    "jsonData.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed80483-3b93-4cc1-8598-f3f73328f5ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- code: long (nullable = true)\n |-- message: string (nullable = true)\n |-- restaurants: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- restaurant: struct (nullable = true)\n |    |    |    |-- R: struct (nullable = true)\n |    |    |    |    |-- res_id: long (nullable = true)\n |    |    |    |-- apikey: string (nullable = true)\n |    |    |    |-- average_cost_for_two: long (nullable = true)\n |    |    |    |-- cuisines: string (nullable = true)\n |    |    |    |-- currency: string (nullable = true)\n |    |    |    |-- deeplink: string (nullable = true)\n |    |    |    |-- establishment_types: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- featured_image: string (nullable = true)\n |    |    |    |-- has_online_delivery: long (nullable = true)\n |    |    |    |-- has_table_booking: long (nullable = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- is_delivering_now: long (nullable = true)\n |    |    |    |-- location: struct (nullable = true)\n |    |    |    |    |-- address: string (nullable = true)\n |    |    |    |    |-- city: string (nullable = true)\n |    |    |    |    |-- city_id: long (nullable = true)\n |    |    |    |    |-- country_id: long (nullable = true)\n |    |    |    |    |-- latitude: string (nullable = true)\n |    |    |    |    |-- locality: string (nullable = true)\n |    |    |    |    |-- locality_verbose: string (nullable = true)\n |    |    |    |    |-- longitude: string (nullable = true)\n |    |    |    |    |-- zipcode: string (nullable = true)\n |    |    |    |-- menu_url: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- offers: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- photos_url: string (nullable = true)\n |    |    |    |-- price_range: long (nullable = true)\n |    |    |    |-- switch_to_order_menu: long (nullable = true)\n |    |    |    |-- thumb: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- user_rating: struct (nullable = true)\n |    |    |    |    |-- aggregate_rating: string (nullable = true)\n |    |    |    |    |-- rating_color: string (nullable = true)\n |    |    |    |    |-- rating_text: string (nullable = true)\n |    |    |    |    |-- votes: string (nullable = true)\n |-- results_found: long (nullable = true)\n |-- results_shown: long (nullable = true)\n |-- results_start: string (nullable = true)\n |-- status: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "jsonData.printSchema()\n",
    "#whenever data type will be array then we need to do flattnaning of that json if data type is struct then we can access any column inside that struct with dot \n",
    "# whereever data type will be array we need to exploide that much times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a586572d-5f7f-43d9-bddb-77012ca7f1ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- code: long (nullable = true)\n |-- message: string (nullable = true)\n |-- restaurants: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- restaurant: struct (nullable = true)\n |    |    |    |-- R: struct (nullable = true)\n |    |    |    |    |-- res_id: long (nullable = true)\n |    |    |    |-- apikey: string (nullable = true)\n |    |    |    |-- average_cost_for_two: long (nullable = true)\n |    |    |    |-- cuisines: string (nullable = true)\n |    |    |    |-- currency: string (nullable = true)\n |    |    |    |-- deeplink: string (nullable = true)\n |    |    |    |-- establishment_types: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- events_url: string (nullable = true)\n |    |    |    |-- featured_image: string (nullable = true)\n |    |    |    |-- has_online_delivery: long (nullable = true)\n |    |    |    |-- has_table_booking: long (nullable = true)\n |    |    |    |-- id: string (nullable = true)\n |    |    |    |-- is_delivering_now: long (nullable = true)\n |    |    |    |-- location: struct (nullable = true)\n |    |    |    |    |-- address: string (nullable = true)\n |    |    |    |    |-- city: string (nullable = true)\n |    |    |    |    |-- city_id: long (nullable = true)\n |    |    |    |    |-- country_id: long (nullable = true)\n |    |    |    |    |-- latitude: string (nullable = true)\n |    |    |    |    |-- locality: string (nullable = true)\n |    |    |    |    |-- locality_verbose: string (nullable = true)\n |    |    |    |    |-- longitude: string (nullable = true)\n |    |    |    |    |-- zipcode: string (nullable = true)\n |    |    |    |-- menu_url: string (nullable = true)\n |    |    |    |-- name: string (nullable = true)\n |    |    |    |-- offers: array (nullable = true)\n |    |    |    |    |-- element: string (containsNull = true)\n |    |    |    |-- photos_url: string (nullable = true)\n |    |    |    |-- price_range: long (nullable = true)\n |    |    |    |-- switch_to_order_menu: long (nullable = true)\n |    |    |    |-- thumb: string (nullable = true)\n |    |    |    |-- url: string (nullable = true)\n |    |    |    |-- user_rating: struct (nullable = true)\n |    |    |    |    |-- aggregate_rating: string (nullable = true)\n |    |    |    |    |-- rating_color: string (nullable = true)\n |    |    |    |    |-- rating_text: string (nullable = true)\n |    |    |    |    |-- votes: string (nullable = true)\n |-- results_found: long (nullable = true)\n |-- results_shown: long (nullable = true)\n |-- results_start: string (nullable = true)\n |-- status: string (nullable = true)\n |-- new_restaurants: struct (nullable = true)\n |    |-- restaurant: struct (nullable = true)\n |    |    |-- R: struct (nullable = true)\n |    |    |    |-- res_id: long (nullable = true)\n |    |    |-- apikey: string (nullable = true)\n |    |    |-- average_cost_for_two: long (nullable = true)\n |    |    |-- cuisines: string (nullable = true)\n |    |    |-- currency: string (nullable = true)\n |    |    |-- deeplink: string (nullable = true)\n |    |    |-- establishment_types: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- events_url: string (nullable = true)\n |    |    |-- featured_image: string (nullable = true)\n |    |    |-- has_online_delivery: long (nullable = true)\n |    |    |-- has_table_booking: long (nullable = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- is_delivering_now: long (nullable = true)\n |    |    |-- location: struct (nullable = true)\n |    |    |    |-- address: string (nullable = true)\n |    |    |    |-- city: string (nullable = true)\n |    |    |    |-- city_id: long (nullable = true)\n |    |    |    |-- country_id: long (nullable = true)\n |    |    |    |-- latitude: string (nullable = true)\n |    |    |    |-- locality: string (nullable = true)\n |    |    |    |-- locality_verbose: string (nullable = true)\n |    |    |    |-- longitude: string (nullable = true)\n |    |    |    |-- zipcode: string (nullable = true)\n |    |    |-- menu_url: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- offers: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- photos_url: string (nullable = true)\n |    |    |-- price_range: long (nullable = true)\n |    |    |-- switch_to_order_menu: long (nullable = true)\n |    |    |-- thumb: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user_rating: struct (nullable = true)\n |    |    |    |-- aggregate_rating: string (nullable = true)\n |    |    |    |-- rating_color: string (nullable = true)\n |    |    |    |-- rating_text: string (nullable = true)\n |    |    |    |-- votes: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "jsonData.select(\"*\",explode(\"restaurants\").alias(\"new_restaurants\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c5ec81-5e45-4185-93be-7447097eeeb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- code: long (nullable = true)\n |-- message: string (nullable = true)\n |-- results_found: long (nullable = true)\n |-- results_shown: long (nullable = true)\n |-- results_start: string (nullable = true)\n |-- status: string (nullable = true)\n |-- new_restaurants: struct (nullable = true)\n |    |-- restaurant: struct (nullable = true)\n |    |    |-- R: struct (nullable = true)\n |    |    |    |-- res_id: long (nullable = true)\n |    |    |-- apikey: string (nullable = true)\n |    |    |-- average_cost_for_two: long (nullable = true)\n |    |    |-- cuisines: string (nullable = true)\n |    |    |-- currency: string (nullable = true)\n |    |    |-- deeplink: string (nullable = true)\n |    |    |-- establishment_types: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- events_url: string (nullable = true)\n |    |    |-- featured_image: string (nullable = true)\n |    |    |-- has_online_delivery: long (nullable = true)\n |    |    |-- has_table_booking: long (nullable = true)\n |    |    |-- id: string (nullable = true)\n |    |    |-- is_delivering_now: long (nullable = true)\n |    |    |-- location: struct (nullable = true)\n |    |    |    |-- address: string (nullable = true)\n |    |    |    |-- city: string (nullable = true)\n |    |    |    |-- city_id: long (nullable = true)\n |    |    |    |-- country_id: long (nullable = true)\n |    |    |    |-- latitude: string (nullable = true)\n |    |    |    |-- locality: string (nullable = true)\n |    |    |    |-- locality_verbose: string (nullable = true)\n |    |    |    |-- longitude: string (nullable = true)\n |    |    |    |-- zipcode: string (nullable = true)\n |    |    |-- menu_url: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- offers: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- photos_url: string (nullable = true)\n |    |    |-- price_range: long (nullable = true)\n |    |    |-- switch_to_order_menu: long (nullable = true)\n |    |    |-- thumb: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- user_rating: struct (nullable = true)\n |    |    |    |-- aggregate_rating: string (nullable = true)\n |    |    |    |-- rating_color: string (nullable = true)\n |    |    |    |-- rating_text: string (nullable = true)\n |    |    |    |-- votes: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "jsonData.select(\"*\",explode(\" \").alias(\"new_restaurants\")).drop(\"restaurants\").printSchema()\n",
    "#now new restaurant data type is struct now if we have to select any column inside many  structs we can select with dot inside dot inside go til; that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b70a1fb4-a39a-4d1e-b554-d0da0ed86c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|  res_id|\n+--------+\n|17066603|\n|17059541|\n|17064405|\n|17057797|\n|17057591|\n|17064266|\n|17060516|\n|17060320|\n|17059060|\n|17059012|\n|17060869|\n|17061231|\n|17058534|\n|17057925|\n|17064031|\n|17061237|\n|17061253|\n|17061296|\n|17061205|\n|17057397|\n+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "jsonData = spark.read.format(\"json\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"mode\",\"permissive\")\\\n",
    ".option(\"inferschema\",\"true\")\\\n",
    ".option(\"multiline\",\"true\")\\\n",
    ".load(\"/FileStore/tables/resturant_json_data.json\")\n",
    "jsonData.select(\"*\",explode(\"restaurants\").alias(\"new_restaurants\")).drop(\"restaurants\")\\\n",
    "    .select(*,\"new_restaurants.restaurant.R.res_id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18db6468-7162-4e35-a686-c81c46abda7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------------+-------------+-------------+------+--------+----------------------+--------------------+\n|code|message|results_found|results_shown|results_start|status|  res_id|newestablishment_types|             newname|\n+----+-------+-------------+-------------+-------------+------+--------+----------------------+--------------------+\n|null|   null|         6835|           20|            1|  null|17066603|                  null|            The Coop|\n|null|   null|         6835|           20|            1|  null|17059541|                  null|Maggiano's Little...|\n|null|   null|         6835|           20|            1|  null|17064405|                  null|Tako Cheena by Po...|\n|null|   null|         6835|           20|            1|  null|17057797|                  null|Bosphorous Turkis...|\n|null|   null|         6835|           20|            1|  null|17057591|                  null|Bahama Breeze Isl...|\n|null|   null|         6835|           20|            1|  null|17064266|                  null|Hawkers Asian Str...|\n|null|   null|         6835|           20|            1|  null|17060516|                  null|Seasons 52 Fresh ...|\n|null|   null|         6835|           20|            1|  null|17060320|                  null|Raglan Road Irish...|\n|null|   null|         6835|           20|            1|  null|17059060|                  null|           Hillstone|\n|null|   null|         6835|           20|            1|  null|17059012|                  null|Hollerbach's Will...|\n|null|   null|         6835|           20|            1|  null|17060869|                  null|     Texas de Brazil|\n|null|   null|         6835|           20|            1|  null|17061231|                  null|    The Ravenous Pig|\n|null|   null|         6835|           20|            1|  null|17058534|                  null|    Earl of Sandwich|\n|null|   null|         6835|           20|            1|  null|17057925|                  null|    Caf Tu Tu Tango|\n|null|   null|         6835|           20|            1|  null|17064031|                  null|Tibby's New Orlea...|\n|null|   null|         6835|           20|            1|  null|17061237|                  null|Cevche Tapas Bar...|\n|null|   null|         6835|           20|            1|  null|17061253|                  null| Ethos Vegan Kitchen|\n|null|   null|         6835|           20|            1|  null|17061296|                  null|Pom Pom's Teahous...|\n|null|   null|         6835|           20|            1|  null|17061205|                  null|     Yellow Dog Eats|\n|null|   null|         6835|           20|            1|  null|17057397|                  null|              'Ohana|\n+----+-------+-------------+-------------+-------------+------+--------+----------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    " jsonData = spark.read.format(\"json\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"mode\",\"permissive\")\\\n",
    ".option(\"inferschema\",\"true\")\\\n",
    ".option(\"multiline\",\"true\")\\\n",
    ".load(\"/FileStore/tables/resturant_json_data.json\")\n",
    "jsonData.select(\"*\",explode(\"restaurants\").alias(\"new_restaurants\")).drop(\"restaurants\").select(\"*\",\"new_restaurants.restaurant.R.res_id\",explode_outer(\"new_restaurants.restaurant.establishment_types\").alias(\"newestablishment_types\"),col(\"new_restaurants.restaurant.name\").alias(\"newname\")).drop(\"new_restaurants\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f71d1c24-1e0e-411f-bf06-806be7d8e37c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PYSPARK_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
